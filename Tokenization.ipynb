{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaca8c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc14d77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\katenam\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2503c8",
   "metadata": {},
   "source": [
    "## 단어 토큰화 예제#1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797f141c",
   "metadata": {},
   "source": [
    "### word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aae0f1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363e0959",
   "metadata": {},
   "source": [
    "### wordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f80b30a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "print(WordPunctTokenizer().tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb94bd6",
   "metadata": {},
   "source": [
    "### text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c1609eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "print(text_to_word_sequence(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffa0fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92878895",
   "metadata": {},
   "source": [
    "## 단어 토큰화 예제#2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff1e7c7",
   "metadata": {},
   "source": [
    "### word_tokenize (NLTK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6aff82c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', '.', 'It', \"'s\", 'good', 'to', 'see', 'you', '.', 'Let', \"'s\", 'start', 'our', 'text', 'mining', 'class', '!']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(\"Hello everyone. It's good to see you. Let's start our text mining class!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f0b2b2",
   "metadata": {},
   "source": [
    "### wordPunctTokenizer (NLTK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b877b610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', '.', 'It', \"'\", 's', 'good', 'to', 'see', 'you', '.', 'Let', \"'\", 's', 'start', 'our', 'text', 'mining', 'class', '!']\n"
     ]
    }
   ],
   "source": [
    "print(WordPunctTokenizer().tokenize(\"Hello everyone. It's good to see you. Let's start our text mining class!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94ddebb",
   "metadata": {},
   "source": [
    "### text_to_word_sequence (Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3680595a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'everyone', \"it's\", 'good', 'to', 'see', 'you', \"let's\", 'start', 'our', 'text', 'mining', 'class']\n"
     ]
    }
   ],
   "source": [
    "print(text_to_word_sequence(\"Hello everyone. It's good to see you. Let's start our text mining class!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2120e133",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e49cd67",
   "metadata": {},
   "source": [
    "## 단어 토큰화 예제#3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5680691b",
   "metadata": {},
   "source": [
    "### word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d989dfe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕하세요', ',', '여러분', ',', '만나서', '반갑습니다', '.', '이제', '텍스트', '마이닝', '클래스를', '시작해봅시다', '!']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(\"안녕하세요, 여러분, 만나서 반갑습니다. 이제 텍스트 마이닝 클래스를 시작해봅시다!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef6fc7f",
   "metadata": {},
   "source": [
    "### wordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76132c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕하세요', ',', '여러분', ',', '만나서', '반갑습니다', '.', '이제', '텍스트', '마이닝', '클래스를', '시작해봅시다', '!']\n"
     ]
    }
   ],
   "source": [
    "print(WordPunctTokenizer().tokenize(\"안녕하세요, 여러분, 만나서 반갑습니다. 이제 텍스트 마이닝 클래스를 시작해봅시다!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a31ae3",
   "metadata": {},
   "source": [
    "### text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8b48646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕하세요', '여러분', '만나서', '반갑습니다', '이제', '텍스트', '마이닝', '클래스를', '시작해봅시다']\n"
     ]
    }
   ],
   "source": [
    "print(text_to_word_sequence(\"안녕하세요, 여러분, 만나서 반갑습니다. 이제 텍스트 마이닝 클래스를 시작해봅시다!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d864bb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "형태소: ['안녕하세요', ',', '여러분', ',', '만나서', '반갑습니다', '.', '이제', '텍스트', '마', '이닝', '클래스', '를', '시작', '해봅시다', '!']\n",
      "\n",
      "명사: ['여러분', '이제', '텍스트', '마', '이닝', '클래스', '시작']\n",
      "\n",
      "품사: [('안녕하세요', 'Adjective'), (',', 'Punctuation'), ('여러분', 'Noun'), (',', 'Punctuation'), ('만나서', 'Verb'), ('반갑습니다', 'Adjective'), ('.', 'Punctuation'), ('이제', 'Noun'), ('텍스트', 'Noun'), ('마', 'Noun'), ('이닝', 'Noun'), ('클래스', 'Noun'), ('를', 'Josa'), ('시작', 'Noun'), ('해봅시다', 'Verb'), ('!', 'Punctuation')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "t = Okt()\n",
    "\n",
    "sentence = \"안녕하세요, 여러분, 만나서 반갑습니다. 이제 텍스트 마이닝 클래스를 시작해봅시다!\"\n",
    "\n",
    "print()\n",
    "print('형태소:',t.morphs(sentence))\n",
    "print()\n",
    "print('명사:',t.nouns(sentence))\n",
    "print()\n",
    "print('품사:',t.pos(sentence))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27988f67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2308c822",
   "metadata": {},
   "source": [
    "### 문장 토큰화 예제_영어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1156323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello everyone.', \"It's good to see you.\", \"Let's start our text mining class!\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sentence = \"Hello everyone. It's good to see you. Let's start our text mining class!\"\n",
    "\n",
    "print(sent_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd5e70e",
   "metadata": {},
   "source": [
    "### 문장 토큰화 예제_한국어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef693087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕하세요, 여러분.', '만나서 반갑습니다.', '이제 텍스트마이닝 클래스를 시작해봅시다!']\n"
     ]
    }
   ],
   "source": [
    "sentence_kor = \"안녕하세요, 여러분. 만나서 반갑습니다. 이제 텍스트마이닝 클래스를 시작해봅시다!\"\n",
    "\n",
    "print(sent_tokenize(sentence_kor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15474496",
   "metadata": {},
   "source": [
    "#### NLTK는 영어 학습 데이터에 대해 사전학습된 모델을 사용해 토큰화한다. 다른 언어에 대해 문장 토큰화를 하려면 사전 학습된 모델을 지정해 불어 올 수 있다.\n",
    "#### NLTK에는 한글에 대해 사전학습된 모델이 없다. 그러나 문장 토큰화는 각 문장의 끝에 있는 마침표 등을 기준으로 분리하도록 학습되어 있으므로, 영어로 학습된 모델도 한국어에 대해 어느 정도 잘 작동할 것으로 예측할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61a8d80",
   "metadata": {},
   "source": [
    "## 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530ee3c2",
   "metadata": {},
   "source": [
    "- ### word_tokenize와는 달리 WordPunctTokenizer는 is, ', s의 세 토큰으로 분리하는 것을 볼 수 있다. 이는 두 토크나이저가 서로 다른 알고리즘에 기반하기 때문이다.\n",
    "- ### 케라스의 text_to_word_sequence는 기본적으로 모든 알파벳을 소문자로 바꾸면서 마침표나 컴마, 느낌표 등의 구두점을 제거한다. 하지만 it's와 let's 같은 경우 아포스트로피는 보존하는 것을 볼 수 있다.\n",
    "- ### 영어를 기준으로 설명할 때에는 단어 토큰화를 단어 단위로 분리한다고 설명할 수 있으나, 한글을 기준으로 할 때에는 이 설명이 명확하지 않다.\n",
    "- ### 한글을 대상으로 하는 토큰화는 엄밀하게 말하자면 의미를 가지는 최소단위, 즉 형태소로 텍스트를 분리하는 것을 의미한다. 그런 의미에서 '안녕하세요'의 경우 '안녕'과 '하세요'를 분리하는 것이 더 맞을 것이다.\n",
    "- ### 영어는 보통 모든 단어를 공백으로 분리할 수 있어 어렵지 않게 단어 토큰화할 수 있지만, 한국어에서는 의미를 이루는 최소 단위가 공백 없이 붙어 있는 경우가 많아서 공백을 이용한 분리만으로는 부족하게 느껴진다.\n",
    "- ### 공배만으로 토큰화가 잘 되지 않는다면 새로운 방법으로 단어를 분리해야 하고 이와 같은 작업을 단어 분할(word segmentation)이라고 한다.\n",
    "- ### 한국어 텍스트를 정확하게 토큰화하려면 다양한 단어 분할방법을 적용할 필요가 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
