{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e80114e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from konlpy.tag import Okt\n",
    "from nltk import Text\n",
    "from matplotlib import font_manager, rc\n",
    "from wordcloud import WordCloud\n",
    "from selenium.webdriver.common.by import By\n",
    " \n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb7e8cf",
   "metadata": {},
   "source": [
    "### url 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cdbda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/katenam/chromedriver.exe\" # 웹드라이버 실행\n",
    " \n",
    "driver = webdriver.Chrome(path) # 드라이버 경로 설정\n",
    "url_list_1 = [] # 블로그 url을 저장하기 위한 변수\n",
    "content_list = \"\" # 블로그 content를 누적하기 위한 변수\n",
    "\n",
    " \n",
    "for i in range(1, 15):  # 1~2페이지까지의 블로그 내용을 읽어옴\n",
    "    url = 'https://section.cafe.naver.com/ca-fe/home/search/articles?q=%EB%A1%AF%EB%8D%B0%EC%98%A8&p='+ str(i) + '&pr=7&ps=2020.07.01&pe=2020.08.31'\n",
    "    driver.get(url)\n",
    "    time.sleep(0.5) # 오류 방지 sleep\n",
    " \n",
    "    for j in range(1, 3):\n",
    "        article = '.item_subject'\n",
    "        articles = driver.find_elements_by_css_selector(article)\n",
    "        for i in articles:\n",
    "            article_url = i.get_attribute('href')\n",
    "            url_list_1.append(article_url)\n",
    " \n",
    "print(\"url 수집 끝, 해당 url 데이터 크롤링\")\n",
    "print(len(url_list_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a229465",
   "metadata": {},
   "source": [
    "### url 들어가서 날짜, 내용, 댓글 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8d0630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 내용 담을 리스트\n",
    "result_1 = []\n",
    "\n",
    "for u in tqdm(url_list_1):\n",
    "    \n",
    "    driver = webdriver.Chrome('C:/Users/katenam/chromedriver.exe')\n",
    "    url = (u)\n",
    "    driver.get(url)\n",
    "\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    driver.switch_to.frame(\"cafe_main\")\n",
    "    container= {}\n",
    "\n",
    "    # 날짜 크롤링\n",
    "    date_list = []\n",
    "    dates = driver.find_elements_by_css_selector('.date')\n",
    "    for a in dates:\n",
    "        d = a.text\n",
    "        date_list.append(d)\n",
    "    \n",
    "    # 내용 크롤링\n",
    "    content_list = []\n",
    "    content = driver.find_elements_by_css_selector('.se-component.se-text.se-l-default')\n",
    "    for i in content:\n",
    "        c = i.text\n",
    "        content_list.append(c)\n",
    "    content_str = ' '.join(content_list)\n",
    "    \n",
    "    # 댓글 크롤링\n",
    "    comment_list = []\n",
    "    comment = driver.find_elements_by_css_selector('.comment_text_box')\n",
    "    for j in comment:\n",
    "        co = j.text\n",
    "        comment_list.append(co)\n",
    "    comment_str = ' '.join(comment_list)\n",
    "    \n",
    "\n",
    "    container['url'] = url\n",
    "    container['date'] = date_list\n",
    "    container['content'] = content_str\n",
    "    container['comment'] = comment_str\n",
    "    \n",
    "    result_1.append(container)\n",
    "    print(container)\n",
    "\n",
    "    # 각각의 글은 dict라는 딕셔너리에 담음\n",
    "#     dict[i] = target_info\n",
    "#     time.sleep(1)\n",
    "\n",
    "#     # 크롤링 성공하면 글 제목을 출력\n",
    "#     print(i)\n",
    "\n",
    "    # 글 하나 크롤링 후 크롬 창 닫기\n",
    "    driver.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dd3060",
   "metadata": {},
   "source": [
    "### 크롤링 결과 데이터 프레임으로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9e7223",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(result_1)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aef01c",
   "metadata": {},
   "source": [
    "### 나눠서 크롤링을 했다면 concat을 사용하여 밑으로 이어 붙이기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e36a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([crawling1,crawling2])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc866f7b",
   "metadata": {},
   "source": [
    "### csv로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25638c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(\"lpay_crawling.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
